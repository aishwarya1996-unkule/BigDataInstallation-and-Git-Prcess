  Install hive jar file from https://downloads.apache.org/hive/hive-1.2.2/
  
  444  tar -xvf /home/user/Downloads/apache-hive-1.2.2-bin.tar.gz 
  445  cp conf/hive-env.sh.template conf/hive-env.sh
  446  cp apache-hive-1.2.2-bin/conf/hive-env.sh.template apache-hive-1.2.2-bin/conf/hive-env.sh
  447  sudo nano apache-hive-1.2.2-bin/conf/hive-env.sh
  export HADOOP_HOME=$HOME/hadoop-2.8.5
  
  448  hadoop fs -mkdir -p /tmp/hive
  449  hadoop fs -chmod 777 /tmp/hive
  450  sudo nano apache-hive-1.2.2-bin/conf/hive-site.xml
  <?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
			<name>javax.jdo.option.ConnectionURL</name>
			<value>jdbc:derby:;databaseName=/home/user/apache-hive-1.2.2-bin/metastore_db;create=true</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionDriverName</name>
			<value>org.apache.derby.jdbc.EmbeddedDriver</value>
		</property>
		<property>
			<name>hive.metastore.uris</name>
			<value>thrift://0.0.0.0:9083</value>
		</property>
		<property>
			<name>javax.jdo.PersistenceManagerFactoryClass</name>
			<value>org.datanucleus.api.jdo.JDOPersistenceManagerFactory</value>
		</property>
		<property>
			<name>hive.metastore.warehouse.dir</name>
			<value>hdfs://localhost:9000/user/hive/warehouse</value>
		</property>
		<property>
			<name>hive.server2.enable.doAs</name>
			<value>false</value>
		</property>
		<property>
			<name>hive.support.concurrency</name>
			<value>true</value>
		</property>
		<property>
			<name>hive.enforce.bucketing</name>
			<value>true</value>
		</property>
		<property>
			<name>hive.exec.dynamic.partition.mode</name>
			<value>nonstrict</value>
		</property>
		<property>
			<name>hive.txn.manager</name>
			<value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
		</property>
		<property>
			<name>hive.compactor.initiator.on</name>
			<value>true</value>
		</property>
		<property>
			<name>hive.compactor.worker.threads</name>
			<value>2</value>
		</property>

</configuration>


  460  sudo nano .bashrc
  export HIVE_HOME=/home/user/apache-hive-1.2.2-bin
export PATH="$PATH:$HIVE_HOME/bin"
export SPARK_HOME=/opt/spark
export PATH=/home/user/.nvm/versions/node/v14.16.0/bin:/home/user/hadoop-3.2.0/bin:/home/user/hadoop-3.2.0/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/bin:/sbin:/opt/spark/bin:/opt/spark/sbin:/opt/spark/bin:/opt/spark/sbin:/home/user/hadoop-2.8.5/sbin:/home/user/hadoop-2.8.5/bin:/home/user/apache-hive-1.2.2-bin/bin:/opt/spark/bin:/opt/spark/sbin
export PYSPARK_PYTHON=/usr/bin/python3

   To check guava jar file
  486  cd apache-hive-1.2.2-bin/
  487  ls
  488  cd lib/
  492  ls
  493  cd ~
  494  cd hadoop-2.8.5/
  495  ls
  502  cd share/
  503  ls
  504  cd hadoop/
  505  ls
  506  cd hdfs/
  507  ls
  508  cd lib/
  509  ls
  510  cd ~
  511  rm apache-hive-1.2.2-bin/lib/guava-14.0.1.jar 
  512  cp hadoop-2.8.5/share/hadoop/hdfs/lib/guava-11.0.2.jar apache-hive-1.2.2-bin/lib/
  
  To check new guava file
  513  cd apache-hive-1.2.2-bin/lib/
  514  ls
  
  Create metastore.
  515  cd ~
  516  schematool -initSchema -dbType derby
  517  cd apache-hive-1.2.2-bin/
  518  schematool -initSchema -dbType derby
  519  cd ~
  
  Create Hive warehouse directory.
	hadoop fs -mkdir -p /user/hive/warehouse
	hadoop fs -chmod g+w /user/hive/warehouse
	
  Start Hive metastore service.
	hive --service metastore
	* Then test on new terminal.
	netstat -tln | grep "9083"
	jps
	
  544  hive
  
  *******************************************************************************************************************************
  Download spark without hadoop jar from https://archive.apache.org/dist/spark/spark-2.4.4/
  
  https://phoenixnap.com/kb/install-spark-on-ubuntu-follow the step for pyspark
  
  545  tar -xvf ~/Downloads/pyspark-2.4.4.tar.gz 
  546  echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
  547  echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.bashrc
  548  echo "export PYSPARK_PYTHON=/usr/bin/python3" >> ~/.bashrc
  549  source ~/.bashrc
  550  start-master.sh
  551  start-master.sh --port 7072 --webui-port 8082
  552  start-worker.sh spark://user-Latitude-3420:7077
  553  spark-shell
  554  pyspark
  
  OR
  tar -xvf ~/Downloads/spark-2.4.4-bin-without-hadoop.tgz
  Go to spark-env.sh and insert - export SPARK_DIST_CLASSPATH=$(hadoop classpath)
  
  Open .bashrc and add -->
  export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/
export PATH=$PATH:$JAVA_HOME/bin
export SPARK_HOME=/home/user/spark-2.4.4-bin-without-hadoop
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

source ~/.bashrc
start-master.sh
  551  start-master.sh --port 7072 --webui-port 8082
  552  start-worker.sh spark://user-Latitude-3420:7077
  553  spark-shell
  554  pyspark
  
  

