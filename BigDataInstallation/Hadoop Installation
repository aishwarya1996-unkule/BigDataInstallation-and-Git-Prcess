Download hadoop jar from https://archive.apache.org/dist/hadoop/common/hadoop-2.8.5/

380  sudo apt update
  381  sudo apt list --upgradable 
  382  ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  383  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  384  chmod 0600 ~/.ssh/authorized_keys
  385  ssh localhost
  386  wget https://archive.apache.org/dist/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz
  387  tar xzf hadoop-2.8.5.tar.gz 
  388  hdfs fs -mkdir user
  389  sudo hdfs fs -mkdir /user
  390  hdfs fs -mkdir /user
  391  hdfs dfs -mkdir /user
  392  hadoop fs -mkdir user
  393  jps
  394  stop-yarn.sh
  395  ./stop-yarn.sh
  396  start-dfs.sh
  397  jps
  sudo nano $.bashrc
export HADOOP_HOME=$HOME/hadoop-2.8.5
alias python='python3'
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS"-Djava.library.path=$HADOOP_HOME/lib/nativ"

export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
  
  398  sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
  export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

  399  sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
  <configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
        <description>The name of the default file system></description>
    </property>
</configuration>

  400  sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
  <property>
  <name>dfs.data.dir</name>
  <value>/home/user/dfsdata/namenode</value>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>/home/user/dfsdata/datanode</value>
</property>
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>


  401  sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
  <configuration>
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

</configuration>

  402  sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
  <configuration>

<!-- Site specific YARN configuration properties -->
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>127.0.0.1</value>
</property>
<property>
  <name>yarn.acl.enable</name>
  <value>0</value>
</property>
<property>
  <name>yarn.nodemanager.env-whitelist</name>
  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>


</configuration>

  403  hdfs namenode -format
  404  ./start-dfs.sh
  405  start-dfs.sh
  406  jps
  407  start-yarn.sh
  408  jps
  409  sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
  410  sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
  411  hdfs namenode -format
  412  start-dfs.sh
  413  jps
  414  start-yarn.sh
  415  jps
  416  cd /usr/lib
  417  ls
  418  cd ~
  419  cd hadoop-2.8.5/sbin/
  420  ls
  421  jps
  422  stop-yarn.sh
  423  stop-dfs.sh
  424  jps
  425  cd ~
  426  start-dfs.sh
  427  jps
  428  stop-dfs.sh
  429  jps
  430  start-dfs.sh
  431  start-yarn.sh
  432  jps
  433  hadoop fs -mkdir shubham
  434  hadoop fs -ls shubham
  435  hadoop fs -mkdir /shubham
  436  hadoop fs -ls /shubham
  437  hadoop fs -put file.txt 
  438  hadoop fs -put file.txt /
  439  cat > file3.txt
  440  hadoop fs -put file3.txt /shubham
  441  hadoop fs -rm file.txt
  442  hadoop fs -rm /file.txt
  443  pwd

